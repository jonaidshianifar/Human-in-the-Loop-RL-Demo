<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Reinforcement Learning 101 ‚Äì Human-Reward Q-Learning Demo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: #020617;
      --card-bg: #020617;
      --card-border: #1f2937;
      --text: #e5e7eb;
      --text-muted: #9ca3af;
      --accent: #3b82f6;
      --accent-soft: rgba(59,130,246,0.2);
      --danger: #ef4444;
      --success: #22c55e;
      --neutral: #4b5563;
      --shadow: 0 18px 40px rgba(15,23,42,0.9);
      --card-radius: 14px;
      --transition-fast: 0.2s ease;
    }

    * {
      box-sizing: border-box;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 0;
      background: var(--bg);
      color: var(--text);
      display: flex;
      flex-direction: column;
      align-items: center;
      min-height: 100vh;
    }

    header {
      padding: 12px 16px 8px;
      width: 100%;
      max-width: 1100px;
      display: flex;
      align-items: center;
      justify-content: space-between;
      gap: 10px;
    }

    .title-block {
      display: flex;
      flex-direction: column;
      gap: 2px;
    }

    h1 {
      margin: 0;
      font-size: clamp(1.25rem, 2.6vw, 1.8rem);
      letter-spacing: 0.02em;
    }

    .subtitle {
      font-size: 0.85rem;
      color: var(--text-muted);
    }

    .header-actions {
      display: flex;
      gap: 8px;
      align-items: center;
      flex-wrap: wrap;
      justify-content: flex-end;
    }

    main {
      padding: 0 16px 10px;
      width: 100%;
      max-width: 1100px;
    }

    .card {
      background: radial-gradient(circle at top left, rgba(148,163,184,0.08), var(--card-bg));
      border-radius: var(--card-radius);
      box-shadow: var(--shadow);
      padding: 14px 16px;
      border: 1px solid var(--card-border);
      margin-bottom: 16px;
    }

    .card h2 {
      margin-top: 0;
      margin-bottom: 6px;
      font-size: 0.98rem;
      letter-spacing: 0.04em;
      text-transform: uppercase;
      color: var(--text-muted);
    }

    .card h3 {
      margin-top: 10px;
      margin-bottom: 4px;
      font-size: 0.9rem;
    }

    p {
      font-size: 0.86rem;
      line-height: 1.5;
      color: var(--text-muted);
      margin: 4px 0 8px;
    }

    ul {
      margin: 4px 0 8px;
      padding-left: 1.2rem;
      color: var(--text-muted);
      font-size: 0.86rem;
    }

    li { margin-bottom: 3px; }

    code {
      background: rgba(15,23,42,0.6);
      border-radius: 4px;
      padding: 0 4px;
      font-size: 0.8rem;
    }

    pre {
      background: rgba(15,23,42,0.7);
      border-radius: 8px;
      padding: 8px 10px;
      overflow-x: auto;
      font-size: 0.8rem;
      color: #e5e7eb;
      border: 1px solid var(--card-border);
    }

    footer {
      font-size: 0.74rem;
      color: var(--text-muted);
      padding: 6px 16px 12px;
      width: 100%;
      max-width: 1100px;
      text-align: right;
    }

    footer a {
      color: var(--accent);
      text-decoration: none;
    }

    footer a:hover {
      text-decoration: underline;
    }

    button,
    .header-link-btn {
      padding: 7px 13px;
      border-radius: 999px;
      border: 1px solid transparent;
      cursor: pointer;
      font-size: 0.86rem;
      font-weight: 500;
      background: transparent;
      color: var(--text);
      transition: background var(--transition-fast), transform var(--transition-fast), border-color var(--transition-fast), box-shadow var(--transition-fast), color var(--transition-fast);
    }

    button:active {
      transform: scale(0.97);
    }

    .header-link-btn {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      text-decoration: none;
      border-color: var(--accent-soft);
      background: rgba(15,23,42,0.9);
      color: var(--text-muted);
      white-space: nowrap;
    }

    .header-link-btn:hover {
      background: rgba(15,23,42,1);
    }

    /* Pills (section labels) */
    .pill {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 2px 10px;
      border-radius: 999px;
      font-size: 0.76rem;
      border: 1px solid var(--accent-soft);
      color: var(--text-muted);
      margin-bottom: 6px;
      background: rgba(15,23,42,0.8);
    }

    .pill .emoji {
      font-size: 0.9rem;
    }

    /* Research papers collapsible section */
    .paper-list {
      margin-top: 6px;
      display: flex;
      flex-direction: column;
      gap: 6px;
    }

    .paper-item {
      border-radius: 10px;
      border: 1px solid var(--card-border);
      background: rgba(15,23,42,0.7);
      overflow: hidden;
    }

    .paper-header {
      width: 100%;
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding: 8px 10px;
      background: transparent;
      border: none;
      cursor: pointer;
      font-size: 0.85rem;
      color: var(--text);
    }

    .paper-title {
      text-align: left;
      font-weight: 500;
      flex: 1;
      padding-right: 8px;
    }

    .paper-toggle-icon {
      font-size: 0.75rem;
      color: var(--text-muted);
      transition: transform 0.2s ease;
    }

    .paper-body {
      display: none;
      padding: 6px 10px 10px;
      border-top: 1px solid var(--card-border);
      font-size: 0.84rem;
      color: var(--text-muted);
    }

    .paper-item.open .paper-body {
      display: block;
    }

    .paper-item.open .paper-toggle-icon {
      transform: rotate(180deg);
    }

    .paper-meta {
      font-size: 0.78rem;
      color: var(--text-muted);
      margin-top: -4px;
      margin-bottom: 4px;
    }

    .paper-btn {
      display: inline-block;
      margin: 4px 0 4px;
      padding: 5px 12px;
      border-radius: 999px;
      background: var(--accent-soft);
      color: var(--accent);
      font-size: 0.8rem;
      text-decoration: none;
      transition: opacity 0.2s ease, transform 0.2s ease;
    }

    .paper-btn:hover {
      opacity: 0.9;
      transform: translateY(-1px);
    }

    /* Social Links Section */
    #social-links-card h2 {
      margin-top: 0;
      margin-bottom: 6px;
      font-size: 0.9rem;
      letter-spacing: 0.04em;
      text-transform: uppercase;
      color: var(--text-muted);
    }

    .social-links {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      margin-top: 12px;
    }

    .social-btn {
      display: inline-flex;
      align-items: center;
      gap: 6px;
      padding: 8px 16px;
      border-radius: 999px;
      font-size: 0.85rem;
      font-weight: 500;
      text-decoration: none;
      border: 1px solid var(--accent-soft);
      background: rgba(15,23,42,0.85);
      color: var(--text);
      transition: transform 0.2s ease, opacity 0.2s ease;
    }

    .social-btn:hover {
      opacity: 0.9;
      transform: translateY(-2px);
    }

    @media (max-width: 800px) {
      header {
        flex-direction: column;
        align-items: flex-start;
      }
      footer {
        text-align: left;
      }
    }
  </style>
</head>
<body>
  <header>
    <div class="title-block">
      <h1>Reinforcement Learning 101</h1>
      <div class="subtitle">
        A friendly overview of RL and algorithms like Q-learning, PPO and SAC.
      </div>
    </div>
    <div class="header-actions">
      <a href="index.html" class="header-link-btn">
        <span>‚Üê</span><span>Back to demo</span>
      </a>
    </div>
  </header>

  <main>
    <section class="card">
      <div class="pill"><span class="emoji">ü§ñ</span><span>What is Reinforcement Learning?</span></div>
      <h2>Core idea</h2>
      <p>
        In <strong>Reinforcement Learning (RL)</strong>, an <em>agent</em> interacts with an
        <em>environment</em>. At each time step it:
      </p>
      <ul>
        <li>observes the <strong>state</strong> <code>s</code></li>
        <li>chooses an <strong>action</strong> <code>a</code></li>
        <li>receives a <strong>reward</strong> <code>r</code> and a new state <code>s'</code></li>
      </ul>
      <p>
        The goal: learn a <strong>policy</strong> (a way of choosing actions) that maximizes
        the <em>long-term reward</em>.
      </p>
      <p>Key pieces of the RL puzzle:</p>
      <ul>
        <li><strong>State</strong> ‚Äì what the agent currently sees.</li>
        <li><strong>Action</strong> ‚Äì what the agent can do.</li>
        <li><strong>Reward</strong> ‚Äì scalar signal of ‚Äúhow good was that?‚Äù.</li>
        <li><strong>Policy</strong> ‚Äì mapping from states to actions.</li>
        <li><strong>Value</strong> ‚Äì how good it is to be in a state (or take an action) in the long run.</li>
      </ul>
    </section>

    <section class="card">
      <div class="pill"><span class="emoji">üßÆ</span><span>Value-based RL</span></div>
      <h2>Q-learning (what the demo uses)</h2>
      <p>
        <strong>Q-learning</strong> learns a table or function
        <code>Q(s,a)</code> = ‚Äúexpected future reward if I take action
        <code>a</code> in state <code>s</code> and act well afterwards‚Äù.
      </p>
      <p>The classic update rule is:</p>
      <pre>
Q(s, a) ‚Üê Q(s, a) + Œ± ¬∑ [ r + Œ≥ ¬∑ max<sub>a'</sub> Q(s', a') ‚àí Q(s, a) ]</pre>
      <p>
        In the Human-Reward Q-Learning demo:
      </p>
      <ul>
        <li>Each cell of the grid is a <strong>state</strong>.</li>
        <li>Actions are <code>up, right, down, left</code>.</li>
        <li><strong>You</strong> choose the reward (<code>-1</code>, <code>0</code>, <code>+1</code>).</li>
        <li>The Q-table is updated based on your feedback.</li>
      </ul>
      <p>
        The agent explores with an <strong>Œµ-greedy</strong> policy: most of the time it chooses
        the action with highest Q-value, but with probability <code>Œµ</code> it picks a random action
        to keep exploring.
      </p>
    </section>

    <section class="card">
      <div class="pill"><span class="emoji">üéØ</span><span>Policy-gradient and Actor-Critic</span></div>
      <h2>From tables to neural networks</h2>
      <p>
        For large or continuous state spaces (e.g. images, robots), we can‚Äôt store a Q-value
        for every state‚Äìaction pair. Instead we use <strong>neural networks</strong>.
      </p>
      <h3>Policy-gradient</h3>
      <p>
        A policy network <code>œÄ<sub>Œ∏</sub>(a | s)</code> outputs a distribution over actions.
        We directly adjust the parameters <code>Œ∏</code> to increase expected reward:
      </p>
      <pre>
Œ∏ ‚Üê Œ∏ + Œ± ¬∑ ‚àá<sub>Œ∏</sub> E[ return ]</pre>
      <p>
        This approach naturally handles <strong>continuous actions</strong> (e.g. torques for a robot arm).
      </p>
      <h3>Actor-Critic</h3>
      <p>
        Actor-Critic methods combine:
      </p>
      <ul>
        <li>an <strong>actor</strong> (policy network) ‚Äì decides actions</li>
        <li>a <strong>critic</strong> (value network) ‚Äì estimates how good states/actions are</li>
      </ul>
      <p>
        The critic helps the actor learn more stable updates by providing a better estimate of
        ‚Äúhow good was that trajectory?‚Äù than raw returns.
      </p>
    </section>

    <section class="card">
      <div class="pill"><span class="emoji">üöÄ</span><span>Popular modern algorithms</span></div>
      <h2>PPO ‚Äì Proximal Policy Optimization</h2>
      <p>
        <strong>PPO</strong> is a widely used on-policy actor-critic method.
        It tries to improve the policy while keeping each update
        <em>close</em> to the old policy so learning stays stable.
      </p>
      <ul>
        <li>Uses a <strong>clipped loss</strong> to prevent too-big policy updates.</li>
        <li>Good default choice for many continuous-control tasks.</li>
        <li>Often used in robotics and game AI.</li>
      </ul>

      <h2>SAC ‚Äì Soft Actor-Critic</h2>
      <p>
        <strong>SAC</strong> is an off-policy actor-critic algorithm that adds an
        <strong>entropy bonus</strong> to the objective:
      </p>
      <ul>
        <li>Maximize reward <em>and</em> keep the policy ‚Äúhigh-entropy‚Äù (more random).</li>
        <li>This encourages exploration and often gives very stable learning.</li>
        <li>Works very well for continuous actions (e.g. MuJoCo robots).</li>
      </ul>
      <p>
        In many modern robotic control setups you‚Äôll see SAC or PPO as the main baseline.
      </p>
    </section>

    <section class="card">
      <div class="pill"><span class="emoji">üßë‚Äçüíª</span><span>Where this demo fits</span></div>
      <h2>Human-reward Q-learning</h2>
      <p>
        The ‚ÄúHuman-Reward Q-Learning Demo‚Äù is intentionally tiny:
      </p>
      <ul>
        <li>Small discrete gridworld.</li>
        <li>Tabular Q-learning instead of neural networks.</li>
        <li>The reward is not hard-coded ‚Äî <strong>you provide it interactively</strong>.</li>
      </ul>
      <p>
        But the structure is the same idea as in bigger systems:
      </p>
      <ul>
        <li>state ‚Üí action ‚Üí reward ‚Üí update the policy/value function</li>
        <li>repeat many times ‚Üí the agent‚Äôs behavior improves</li>
      </ul>
      <p>
        You can think of complex algorithms like PPO or SAC as
        ‚Äúmuch smarter, GPU-powered versions‚Äù of this little loop, usually with:
      </p>
      <ul>
        <li>vector states (images, sensor data, joint positions)</li>
        <li>continuous actions (forces, torques)</li>
        <li>neural networks instead of a simple Q-table</li>
      </ul>
      <p>
        Your demo is a great way to make that learning loop visible and interactive.
      </p>
    </section>
  </main>

  <!-- Research papers section -->
  <section class="card" id="papers-card" style="max-width:1100px;width:100%;margin:0 16px 10px;">
    <div class="pill"><span class="emoji">üìÑ</span><span>Read My Research Papers</span></div>
    <h2 style="margin-top:4px;font-size:0.9rem;letter-spacing:0.04em;text-transform:uppercase;color:var(--text-muted);">
      Recent Work & Publications
    </h2>

    <div class="paper-list">
      <article class="paper-item">
  <button class="paper-header" type="button">
    <span class="paper-title">Optimizing Deep Reinforcement Learning for Adaptive Robotic Arm Control</span>
    <span class="paper-toggle-icon">‚ñº</span>
  </button>
  <div class="paper-body">
    <p>
      Explores hyperparameter optimization for SAC and PPO using Tree-structured Parzen Estimator (TPE) on a 7-DOF robotic arm.  
      TPE significantly improves success rates and speeds up convergence (up to ~80% faster training for SAC and ~76% faster for PPO to reach near-optimal reward),  
      highlighting how advanced hyperparameter search boosts deep RL performance in complex robotic tasks.
    </p>
    <p class="paper-meta">
      Jonaid Shianifar, Michael Schukat, Karl Mason<br>
      International Conference on Practical Applications of Agents and Multi-Agent Systems (Springer Nature Switzerland), pp. 293‚Äì304, (2024)
    </p>
    <a href="https://arxiv.org/abs/2407.02503" class="paper-btn">View paper</a>
  </div>
</article>



    </div>
  </section>

  <!-- Social links section -->
  <section class="card" id="social-links-card" style="max-width:1100px;width:100%;margin:0 16px 10px;">
    <h2>Connect With Me</h2>
    <div class="social-links">
      <a href="https://github.com/jonaidshianifar" target="_blank" class="social-btn github">
        <span>üêô</span> GitHub
      </a>

      <a href="https://scholar.google.com/citations?user=tTQUX18AAAAJ&hl=en" target="_blank" class="social-btn scholar">
        <span>üéì</span> Google Scholar
      </a>

      <a href="https://ie.linkedin.com/in/jonaid-shianifar" target="_blank" class="social-btn linkedin">
        <span>üíº</span> LinkedIn
      </a>
    </div>
  </section>

  <footer>
    Made by
    <a href="https://github.com/jonaidshianifar" target="_blank" rel="noopener noreferrer">
      Jonaid Shianifar
    </a>.
  </footer>

  <script>
    // Collapsible paper section
    (function initPaperCollapse() {
      const items = document.querySelectorAll(".paper-item");
      items.forEach((item, idx) => {
        const header = item.querySelector(".paper-header");
        if (!header) return;
        header.addEventListe
